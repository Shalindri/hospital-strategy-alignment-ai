{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISPS Parameter Tuning Experiments\n",
    "\n",
    "This notebook systematically tests and calibrates key parameters in the\n",
    "Hospital Strategy–Action Plan Alignment System (ISPS) to find optimal values.\n",
    "\n",
    "**Experiments:**\n",
    "1. Embedding Model Comparison\n",
    "2. Classification Threshold Sweep\n",
    "3. Ontology Hybrid-Scoring Weight Calibration\n",
    "4. RAG Retrieval `top_k` Tuning\n",
    "\n",
    "All experiments are evaluated against the **58-pair human-annotated ground truth**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys, json, time, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom sklearn.metrics import (\n    f1_score, precision_score, recall_score, accuracy_score,\n    roc_curve, auc, mean_absolute_error, confusion_matrix,\n)\nfrom scipy.stats import pearsonr, spearmanr\n\nwarnings.filterwarnings(\"ignore\")\nsns.set_theme(style=\"whitegrid\", font_scale=1.1)\n\n# Project root\nPROJECT_ROOT = Path(\"..\").resolve()\nsys.path.insert(0, str(PROJECT_ROOT))\n\nDATA_DIR = PROJECT_ROOT / \"data\"\nGT_FILE  = PROJECT_ROOT / \"tests\" / \"ground_truth.json\"\n\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(f\"Ground truth: {GT_FILE.exists()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Ground Truth & Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "with open(GT_FILE) as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "gt_df = pd.DataFrame(ground_truth)\n",
    "print(f\"Ground truth pairs: {len(gt_df)}\")\n",
    "print(f\"Label distribution:\\n{gt_df['alignment_label'].value_counts().sort_index()}\")\n",
    "\n",
    "# Binary labels: 0.7 and 1.0 → aligned (1), 0.0 and 0.4 → not aligned (0)\n",
    "BINARY_THRESHOLD_GT = 0.5\n",
    "gt_df[\"binary_label\"] = (gt_df[\"alignment_label\"] >= BINARY_THRESHOLD_GT).astype(int)\n",
    "print(f\"\\nBinary: {gt_df['binary_label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load strategic plan and action plan texts\n",
    "with open(DATA_DIR / \"strategic_plan.json\") as f:\n",
    "    sp = json.load(f)\n",
    "with open(DATA_DIR / \"action_plan.json\") as f:\n",
    "    ap = json.load(f)\n",
    "\n",
    "def build_objective_text(obj):\n",
    "    parts = [obj.get(\"name\", \"\"), obj.get(\"goal_statement\", \"\")]\n",
    "    for g in obj.get(\"strategic_goals\", []):\n",
    "        parts.append(g.get(\"description\", \"\") if isinstance(g, dict) else str(g))\n",
    "    for kpi in obj.get(\"kpis\", []):\n",
    "        if isinstance(kpi, dict):\n",
    "            parts.append(kpi.get(\"description\", kpi.get(\"metric\", \"\")))\n",
    "        else:\n",
    "            parts.append(str(kpi))\n",
    "    parts.extend(obj.get(\"keywords\", []))\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def build_action_text(act):\n",
    "    parts = [act.get(\"title\", \"\"), act.get(\"description\", \"\"), act.get(\"expected_outcome\", \"\")]\n",
    "    for kpi in act.get(\"kpis\", []):\n",
    "        if isinstance(kpi, dict):\n",
    "            parts.append(kpi.get(\"description\", kpi.get(\"metric\", \"\")))\n",
    "        else:\n",
    "            parts.append(str(kpi))\n",
    "    parts.extend(act.get(\"keywords\", []))\n",
    "    return \" \".join(parts)\n",
    "\n",
    "objective_texts = {obj[\"code\"]: build_objective_text(obj) for obj in sp[\"objectives\"]}\n",
    "action_texts = {act[\"action_number\"]: build_action_text(act) for act in ap[\"actions\"]}\n",
    "\n",
    "print(f\"Objectives: {len(objective_texts)}, Actions: {len(action_texts)}\")\n",
    "print(f\"Sample objective text (first 150 chars): {list(objective_texts.values())[0][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_similarity_matrix(model_name, objective_texts, action_texts):\n    \"\"\"Compute pairwise cosine similarity matrix using a sentence-transformer model.\n    \n    Returns: (sim_matrix, obj_codes, act_nums, perf_info)\n        perf_info contains timing and model metadata.\n    \"\"\"\n    from sentence_transformers import SentenceTransformer\n    \n    # Time model loading\n    t0 = time.perf_counter()\n    model = SentenceTransformer(model_name)\n    load_time = time.perf_counter() - t0\n    \n    obj_codes = sorted(objective_texts.keys())\n    act_nums = sorted(action_texts.keys())\n    \n    # Time encoding\n    t1 = time.perf_counter()\n    obj_embs = model.encode(\n        [objective_texts[c] for c in obj_codes],\n        normalize_embeddings=True, show_progress_bar=False\n    )\n    act_embs = model.encode(\n        [action_texts[n] for n in act_nums],\n        normalize_embeddings=True, show_progress_bar=False\n    )\n    encode_time = time.perf_counter() - t1\n    \n    # Time similarity computation\n    t2 = time.perf_counter()\n    sim_matrix = obj_embs @ act_embs.T\n    sim_time = time.perf_counter() - t2\n    \n    # Model metadata\n    embedding_dim = obj_embs.shape[1]\n    num_params = sum(p.numel() for p in model[0].auto_model.parameters()) / 1e6  # millions\n    \n    perf_info = {\n        \"load_time_s\": round(load_time, 3),\n        \"encode_time_s\": round(encode_time, 3),\n        \"similarity_time_s\": round(sim_time, 6),\n        \"total_time_s\": round(load_time + encode_time + sim_time, 3),\n        \"embedding_dim\": embedding_dim,\n        \"model_params_M\": round(num_params, 1),\n    }\n    \n    return sim_matrix, obj_codes, act_nums, perf_info\n\n\ndef extract_gt_scores(sim_matrix, obj_codes, act_nums, gt_df):\n    \"\"\"Extract predicted scores for ground-truth pairs from the similarity matrix.\"\"\"\n    obj_idx = {c: i for i, c in enumerate(obj_codes)}\n    act_idx = {n: i for i, n in enumerate(act_nums)}\n    \n    preds = []\n    for _, row in gt_df.iterrows():\n        i = obj_idx[row[\"objective_code\"]]\n        j = act_idx[row[\"action_number\"]]\n        preds.append(float(sim_matrix[i, j]))\n    return np.array(preds)\n\n\ndef evaluate_predictions(y_true_cont, y_pred_cont, threshold=0.5):\n    \"\"\"Compute classification & regression metrics.\"\"\"\n    y_true_bin = (y_true_cont >= BINARY_THRESHOLD_GT).astype(int)\n    y_pred_bin = (y_pred_cont >= threshold).astype(int)\n    \n    fpr, tpr, thresholds = roc_curve(y_true_bin, y_pred_cont)\n    roc_auc = auc(fpr, tpr)\n    \n    # Youden's J for optimal threshold\n    j_scores = tpr - fpr\n    best_idx = np.argmax(j_scores)\n    optimal_threshold = thresholds[best_idx]\n    \n    return {\n        \"precision\": precision_score(y_true_bin, y_pred_bin, zero_division=0),\n        \"recall\": recall_score(y_true_bin, y_pred_bin, zero_division=0),\n        \"f1\": f1_score(y_true_bin, y_pred_bin, zero_division=0),\n        \"accuracy\": accuracy_score(y_true_bin, y_pred_bin),\n        \"auc\": roc_auc,\n        \"mae\": mean_absolute_error(y_true_cont, y_pred_cont),\n        \"pearson_r\": pearsonr(y_true_cont, y_pred_cont)[0],\n        \"spearman_rho\": spearmanr(y_true_cont, y_pred_cont)[0],\n        \"optimal_threshold\": optimal_threshold,\n        \"roc_data\": (fpr, tpr),\n    }\n\nprint(\"Helper functions loaded.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1: Embedding Model Comparison\n",
    "\n",
    "Compare different sentence-transformer models to find which produces the\n",
    "best alignment scores against our ground truth.\n",
    "\n",
    "| Model | Dimensions | Speed | Quality |\n",
    "|-------|-----------|-------|--------|\n",
    "| all-MiniLM-L6-v2 (current) | 384 | Fast | Good |\n",
    "| all-MiniLM-L12-v2 | 384 | Medium | Better |\n",
    "| all-mpnet-base-v2 | 768 | Slower | Best |\n",
    "| paraphrase-MiniLM-L6-v2 | 384 | Fast | Good |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CANDIDATE_MODELS = [\n    \"all-MiniLM-L6-v2\",          # current default\n    \"all-MiniLM-L12-v2\",         # deeper MiniLM\n    \"all-mpnet-base-v2\",         # highest quality SBERT\n    \"paraphrase-MiniLM-L6-v2\",   # paraphrase-tuned\n]\n\ny_true = gt_df[\"alignment_label\"].values\nmodel_results = {}\nmodel_perf = {}\n\nfor model_name in CANDIDATE_MODELS:\n    print(f\"\\nEvaluating: {model_name}\")\n    sim_matrix, obj_codes, act_nums, perf_info = compute_similarity_matrix(\n        model_name, objective_texts, action_texts\n    )\n    preds = extract_gt_scores(sim_matrix, obj_codes, act_nums, gt_df)\n    metrics = evaluate_predictions(y_true, preds, threshold=0.5)\n    model_results[model_name] = metrics\n    model_perf[model_name] = perf_info\n    print(f\"  AUC={metrics['auc']:.4f}  F1={metrics['f1']:.4f}  \"\n          f\"MAE={metrics['mae']:.4f}  Pearson={metrics['pearson_r']:.4f}  \"\n          f\"Optimal threshold={metrics['optimal_threshold']:.4f}\")\n    print(f\"  ⏱ Load={perf_info['load_time_s']:.2f}s  Encode={perf_info['encode_time_s']:.3f}s  \"\n          f\"Total={perf_info['total_time_s']:.2f}s  \"\n          f\"Dim={perf_info['embedding_dim']}  Params={perf_info['model_params_M']}M\")\n\nprint(\"\\nDone.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary table — accuracy + performance\nsummary_rows = []\nfor name, m in model_results.items():\n    p = model_perf[name]\n    summary_rows.append({\n        \"Model\": name,\n        \"AUC\": round(m[\"auc\"], 4),\n        \"F1\": round(m[\"f1\"], 4),\n        \"Precision\": round(m[\"precision\"], 4),\n        \"Recall\": round(m[\"recall\"], 4),\n        \"Accuracy\": round(m[\"accuracy\"], 4),\n        \"MAE\": round(m[\"mae\"], 4),\n        \"Pearson r\": round(m[\"pearson_r\"], 4),\n        \"Spearman ρ\": round(m[\"spearman_rho\"], 4),\n        \"Optimal θ\": round(m[\"optimal_threshold\"], 4),\n        \"Dims\": p[\"embedding_dim\"],\n        \"Params (M)\": p[\"model_params_M\"],\n        \"Load (s)\": p[\"load_time_s\"],\n        \"Encode (s)\": p[\"encode_time_s\"],\n        \"Total (s)\": p[\"total_time_s\"],\n    })\n\nsummary_df = pd.DataFrame(summary_rows).set_index(\"Model\")\ndisplay(summary_df.style\n    .highlight_max(axis=0, subset=[\"AUC\", \"F1\", \"Precision\", \"Recall\", \"Accuracy\", \"Pearson r\", \"Spearman ρ\"])\n    .highlight_min(axis=0, subset=[\"MAE\", \"Load (s)\", \"Encode (s)\", \"Total (s)\"]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ROC Curves\nfig, ax = plt.subplots(figsize=(8, 6))\nfor name, m in model_results.items():\n    fpr, tpr = m[\"roc_data\"]\n    ax.plot(fpr, tpr, label=f\"{name} (AUC={m['auc']:.3f})\", linewidth=2)\n\nax.plot([0, 1], [0, 1], \"k--\", alpha=0.3, label=\"Random\")\nax.set_xlabel(\"False Positive Rate\")\nax.set_ylabel(\"True Positive Rate\")\nax.set_title(\"Experiment 1: Embedding Model ROC Comparison\")\nax.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.savefig(\"exp1_roc_curves.png\", dpi=150)\nplt.show()\n\n# Accuracy + Performance side by side\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 1) Classification metrics\nsummary_df[[\"F1\", \"AUC\", \"Accuracy\"]].plot(kind=\"bar\", ax=axes[0], rot=15)\naxes[0].set_title(\"Classification Metrics (higher is better)\")\naxes[0].set_ylim(0, 1)\naxes[0].legend(loc=\"lower right\")\n\n# 2) MAE\nsummary_df[[\"MAE\"]].plot(kind=\"bar\", ax=axes[1], rot=15, color=\"coral\")\naxes[1].set_title(\"Mean Absolute Error (lower is better)\")\n\n# 3) Performance: encode time + model size\nax3 = axes[2]\nx = np.arange(len(summary_df))\nwidth = 0.35\nbars1 = ax3.bar(x - width/2, summary_df[\"Encode (s)\"], width, label=\"Encode time (s)\", color=\"steelblue\")\nax3_r = ax3.twinx()\nbars2 = ax3_r.bar(x + width/2, summary_df[\"Params (M)\"], width, label=\"Parameters (M)\", color=\"salmon\", alpha=0.8)\nax3.set_xticks(x)\nax3.set_xticklabels(summary_df.index, rotation=15, ha=\"right\")\nax3.set_ylabel(\"Encode Time (s)\", color=\"steelblue\")\nax3_r.set_ylabel(\"Parameters (millions)\", color=\"salmon\")\nax3.set_title(\"Performance: Speed & Model Size\")\nlines1, labels1 = ax3.get_legend_handles_labels()\nlines2, labels2 = ax3_r.get_legend_handles_labels()\nax3.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\")\n\nplt.tight_layout()\nplt.savefig(\"exp1_model_comparison.png\", dpi=150)\nplt.show()\n\n# Quality-vs-Speed trade-off scatter\nfig, ax = plt.subplots(figsize=(8, 6))\nfor name in summary_df.index:\n    ax.scatter(summary_df.loc[name, \"Total (s)\"], summary_df.loc[name, \"AUC\"],\n               s=summary_df.loc[name, \"Params (M)\"] * 3, alpha=0.7, edgecolors=\"black\")\n    ax.annotate(name, (summary_df.loc[name, \"Total (s)\"], summary_df.loc[name, \"AUC\"]),\n                textcoords=\"offset points\", xytext=(8, 5), fontsize=9)\nax.set_xlabel(\"Total Time (s) — lower is better\")\nax.set_ylabel(\"AUC — higher is better\")\nax.set_title(\"Quality vs Speed Trade-off (bubble size = model parameters)\")\nplt.tight_layout()\nplt.savefig(\"exp1_quality_vs_speed.png\", dpi=150)\nplt.show()\n\nbest_model = summary_df[\"AUC\"].idxmax()\nfastest_model = summary_df[\"Total (s)\"].idxmin()\nprint(f\"\\n★ Best accuracy:   {best_model} (AUC={summary_df.loc[best_model, 'AUC']:.4f}, {summary_df.loc[best_model, 'Total (s)']:.2f}s)\")\nprint(f\"★ Fastest model:   {fastest_model} (AUC={summary_df.loc[fastest_model, 'AUC']:.4f}, {summary_df.loc[fastest_model, 'Total (s)']:.2f}s)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2: Classification Threshold Sweep\n",
    "\n",
    "Sweep the alignment classification threshold (currently `THRESHOLD_FAIR = 0.45`)\n",
    "to find the value that maximises F1 score against the ground truth.\n",
    "\n",
    "This threshold determines:\n",
    "- Whether an action is classified as \"aligned\" to an objective\n",
    "- Whether an action is an \"orphan\" (below threshold for all objectives)\n",
    "- Edge creation in the Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the best model from Experiment 1 (or default)\nEVAL_MODEL = best_model if 'best_model' in dir() else \"all-MiniLM-L6-v2\"\nprint(f\"Using model: {EVAL_MODEL}\")\n\nsim_matrix, obj_codes, act_nums, _ = compute_similarity_matrix(\n    EVAL_MODEL, objective_texts, action_texts\n)\npreds = extract_gt_scores(sim_matrix, obj_codes, act_nums, gt_df)\n\n# Sweep thresholds from 0.25 to 0.70\nthresholds = np.arange(0.25, 0.71, 0.01)\nsweep_results = []\n\nfor t in thresholds:\n    m = evaluate_predictions(y_true, preds, threshold=t)\n    sweep_results.append({\n        \"threshold\": round(t, 2),\n        \"f1\": m[\"f1\"],\n        \"precision\": m[\"precision\"],\n        \"recall\": m[\"recall\"],\n        \"accuracy\": m[\"accuracy\"],\n    })\n\nsweep_df = pd.DataFrame(sweep_results)\nprint(f\"Swept {len(thresholds)} thresholds from {thresholds[0]:.2f} to {thresholds[-1]:.2f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(sweep_df[\"threshold\"], sweep_df[\"f1\"], \"b-o\", markersize=3, label=\"F1\", linewidth=2)\n",
    "ax.plot(sweep_df[\"threshold\"], sweep_df[\"precision\"], \"g--\", markersize=3, label=\"Precision\", linewidth=1.5)\n",
    "ax.plot(sweep_df[\"threshold\"], sweep_df[\"recall\"], \"r--\", markersize=3, label=\"Recall\", linewidth=1.5)\n",
    "ax.plot(sweep_df[\"threshold\"], sweep_df[\"accuracy\"], \"m:\", markersize=3, label=\"Accuracy\", linewidth=1.5)\n",
    "\n",
    "# Mark current threshold\n",
    "ax.axvline(x=0.45, color=\"orange\", linestyle=\"-.\", alpha=0.7, label=\"Current (0.45)\")\n",
    "\n",
    "# Mark optimal F1 threshold\n",
    "best_idx = sweep_df[\"f1\"].idxmax()\n",
    "best_t = sweep_df.loc[best_idx, \"threshold\"]\n",
    "best_f1 = sweep_df.loc[best_idx, \"f1\"]\n",
    "ax.axvline(x=best_t, color=\"blue\", linestyle=\"-.\", alpha=0.7, label=f\"Optimal ({best_t:.2f})\")\n",
    "ax.scatter([best_t], [best_f1], s=100, c=\"blue\", zorder=5, marker=\"*\")\n",
    "\n",
    "ax.set_xlabel(\"Classification Threshold\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Experiment 2: Threshold Sweep — F1 / Precision / Recall\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"exp2_threshold_sweep.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print top-5 thresholds by F1\n",
    "print(\"\\nTop 5 thresholds by F1:\")\n",
    "print(sweep_df.nlargest(5, \"f1\").to_string(index=False))\n",
    "\n",
    "print(f\"\\n★ Optimal classification threshold: {best_t:.2f} (F1={best_f1:.4f})\")\n",
    "print(f\"  Current threshold: 0.45\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also compute Youden's J statistic from ROC curve\n",
    "y_true_bin = (y_true >= BINARY_THRESHOLD_GT).astype(int)\n",
    "fpr_arr, tpr_arr, roc_thresholds = roc_curve(y_true_bin, preds)\n",
    "j_scores = tpr_arr - fpr_arr\n",
    "youden_idx = np.argmax(j_scores)\n",
    "youden_threshold = roc_thresholds[youden_idx]\n",
    "\n",
    "print(f\"Youden's J optimal threshold: {youden_threshold:.4f}\")\n",
    "print(f\"F1-optimal threshold:         {best_t:.2f}\")\n",
    "print(f\"Current system threshold:     0.45\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3: Ontology Hybrid-Scoring Weight Calibration\n",
    "\n",
    "The ontology mapper uses a hybrid score:\n",
    "\n",
    "```\n",
    "final_score = EMBEDDING_WEIGHT × embedding_sim + (1 - EMBEDDING_WEIGHT) × keyword_score\n",
    "```\n",
    "\n",
    "Currently `EMBEDDING_WEIGHT = 0.6`. We sweep from 0.0 (keyword-only) to 1.0\n",
    "(embedding-only) to find the optimal mix.\n",
    "\n",
    "We evaluate by measuring how well the ontology mappings agree with ground\n",
    "truth alignment — actions mapped to concepts under the correct strategic\n",
    "objective should have higher alignment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from src.ontology_mapper import CONCEPT_KEYWORDS, ONTOLOGY_CONCEPTS\n",
    "\n",
    "# Load model\n",
    "st_model = SentenceTransformer(EVAL_MODEL)\n",
    "\n",
    "# Flatten ontology concepts to get concept texts\n",
    "concept_texts = {}\n",
    "for top_level, children in ONTOLOGY_CONCEPTS.items():\n",
    "    concept_texts[top_level] = top_level.replace(\"_\", \" \")\n",
    "    for child in children:\n",
    "        concept_texts[child] = child.replace(\"_\", \" \")\n",
    "\n",
    "print(f\"Total ontology concepts: {len(concept_texts)}\")\n",
    "print(f\"Top-level concepts: {list(ONTOLOGY_CONCEPTS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute embeddings for all concept texts\n",
    "concept_ids = sorted(concept_texts.keys())\n",
    "concept_embs = st_model.encode(\n",
    "    [concept_texts[c] for c in concept_ids],\n",
    "    normalize_embeddings=True, show_progress_bar=False\n",
    ")\n",
    "\n",
    "# Precompute action embeddings\n",
    "act_nums_sorted = sorted(action_texts.keys())\n",
    "action_emb_list = st_model.encode(\n",
    "    [action_texts[n] for n in act_nums_sorted],\n",
    "    normalize_embeddings=True, show_progress_bar=False\n",
    ")\n",
    "\n",
    "\n",
    "def compute_keyword_score(text, concept_id):\n",
    "    \"\"\"Simple keyword overlap score.\"\"\"\n",
    "    keywords = CONCEPT_KEYWORDS.get(concept_id, [])\n",
    "    if not keywords:\n",
    "        return 0.0\n",
    "    text_lower = text.lower()\n",
    "    matches = sum(1 for kw in keywords if kw.lower() in text_lower)\n",
    "    return min(matches / max(len(keywords), 1), 1.0)\n",
    "\n",
    "\n",
    "def compute_hybrid_mappings(embedding_weight, mapping_threshold=0.55):\n",
    "    \"\"\"Compute ontology mappings with given embedding weight.\n",
    "    \n",
    "    Returns mapping quality metric: how well top-mapped concepts\n",
    "    discriminate between aligned and non-aligned pairs.\n",
    "    \"\"\"\n",
    "    keyword_weight = 1.0 - embedding_weight\n",
    "    \n",
    "    # For each action, find its best-matching concepts\n",
    "    action_concept_scores = {}  # {action_number: {concept_id: score}}\n",
    "    \n",
    "    for idx, act_num in enumerate(act_nums_sorted):\n",
    "        act_emb = action_emb_list[idx]\n",
    "        act_text = action_texts[act_num]\n",
    "        scores = {}\n",
    "        \n",
    "        for cidx, cid in enumerate(concept_ids):\n",
    "            emb_score = float(act_emb @ concept_embs[cidx])\n",
    "            kw_score = compute_keyword_score(act_text, cid)\n",
    "            final = embedding_weight * emb_score + keyword_weight * kw_score\n",
    "            if final >= mapping_threshold:\n",
    "                scores[cid] = final\n",
    "        \n",
    "        action_concept_scores[act_num] = scores\n",
    "    \n",
    "    # Quality metric: average number of valid mappings per action\n",
    "    avg_mappings = np.mean([len(v) for v in action_concept_scores.values()])\n",
    "    \n",
    "    # Coverage: proportion of actions with at least 1 mapping\n",
    "    coverage = np.mean([1 if len(v) > 0 else 0 for v in action_concept_scores.values()])\n",
    "    \n",
    "    # Discrimination: do aligned GT pairs map to more shared concepts?\n",
    "    # (Actions aligned to same objective should share ontology concepts)\n",
    "    discrimination_scores = []\n",
    "    for _, row in gt_df.iterrows():\n",
    "        act_num = row[\"action_number\"]\n",
    "        label = row[\"alignment_label\"]\n",
    "        n_concepts = len(action_concept_scores.get(act_num, {}))\n",
    "        discrimination_scores.append((label, n_concepts))\n",
    "    \n",
    "    disc_df = pd.DataFrame(discrimination_scores, columns=[\"label\", \"n_concepts\"])\n",
    "    aligned_concepts = disc_df[disc_df[\"label\"] >= 0.5][\"n_concepts\"].mean()\n",
    "    unaligned_concepts = disc_df[disc_df[\"label\"] < 0.5][\"n_concepts\"].mean()\n",
    "    \n",
    "    return {\n",
    "        \"avg_mappings\": avg_mappings,\n",
    "        \"coverage\": coverage,\n",
    "        \"aligned_mean_concepts\": aligned_concepts,\n",
    "        \"unaligned_mean_concepts\": unaligned_concepts,\n",
    "    }\n",
    "\n",
    "print(\"Hybrid scoring function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep embedding weight from 0.0 to 1.0\n",
    "weights = np.arange(0.0, 1.05, 0.05)\n",
    "weight_results = []\n",
    "\n",
    "for w in weights:\n",
    "    result = compute_hybrid_mappings(embedding_weight=w)\n",
    "    result[\"embedding_weight\"] = round(w, 2)\n",
    "    weight_results.append(result)\n",
    "\n",
    "weight_df = pd.DataFrame(weight_results)\n",
    "print(weight_df[[\"embedding_weight\", \"avg_mappings\", \"coverage\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Average mappings & coverage\n",
    "ax1 = axes[0]\n",
    "ax1.plot(weight_df[\"embedding_weight\"], weight_df[\"avg_mappings\"], \"b-o\",\n",
    "         markersize=4, label=\"Avg mappings/action\", linewidth=2)\n",
    "ax1_r = ax1.twinx()\n",
    "ax1_r.plot(weight_df[\"embedding_weight\"], weight_df[\"coverage\"], \"r-s\",\n",
    "           markersize=4, label=\"Coverage\", linewidth=2)\n",
    "ax1.axvline(x=0.6, color=\"orange\", linestyle=\"-.\", alpha=0.7, label=\"Current (0.6)\")\n",
    "ax1.set_xlabel(\"Embedding Weight\")\n",
    "ax1.set_ylabel(\"Avg Mappings per Action\", color=\"blue\")\n",
    "ax1_r.set_ylabel(\"Coverage\", color=\"red\")\n",
    "ax1.set_title(\"Mapping Count & Coverage vs Embedding Weight\")\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax1_r.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "# Plot 2: Discrimination\n",
    "ax2 = axes[1]\n",
    "ax2.plot(weight_df[\"embedding_weight\"], weight_df[\"aligned_mean_concepts\"],\n",
    "         \"g-o\", markersize=4, label=\"Aligned pairs\", linewidth=2)\n",
    "ax2.plot(weight_df[\"embedding_weight\"], weight_df[\"unaligned_mean_concepts\"],\n",
    "         \"r-s\", markersize=4, label=\"Unaligned pairs\", linewidth=2)\n",
    "ax2.axvline(x=0.6, color=\"orange\", linestyle=\"-.\", alpha=0.7, label=\"Current (0.6)\")\n",
    "ax2.set_xlabel(\"Embedding Weight\")\n",
    "ax2.set_ylabel(\"Mean Concepts Mapped\")\n",
    "ax2.set_title(\"Concept Discrimination: Aligned vs Unaligned\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"exp3_ontology_weights.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Best weight by coverage\n",
    "best_weight_idx = weight_df[\"coverage\"].idxmax()\n",
    "print(f\"\\n★ Best embedding weight by coverage: {weight_df.loc[best_weight_idx, 'embedding_weight']:.2f}\")\n",
    "print(f\"  Current weight: 0.60\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 4: RAG Retrieval `top_k` Tuning\n",
    "\n",
    "The RAG engine retrieves `top_k` strategic objectives for context when\n",
    "generating recommendations. Currently `top_k = 3`.\n",
    "\n",
    "We measure retrieval quality: for each action in the ground truth, does\n",
    "the correct (declared) objective appear in the top-k retrieved results?\n",
    "\n",
    "This experiment does **not** require an LLM — it only evaluates the\n",
    "retrieval step using ChromaDB embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the alignment similarity matrix to simulate retrieval\n",
    "# For each action, rank objectives by similarity score\n",
    "# Check if the declared objective is within top_k\n",
    "\n",
    "obj_idx_map = {c: i for i, c in enumerate(obj_codes)}\n",
    "act_idx_map = {n: i for i, n in enumerate(act_nums)}\n",
    "\n",
    "# Get declared objective for each action\n",
    "action_declared_obj = {}\n",
    "for act in ap[\"actions\"]:\n",
    "    action_declared_obj[act[\"action_number\"]] = act.get(\"strategic_objective_code\", \"\")\n",
    "\n",
    "topk_values = [1, 2, 3, 4, 5]\n",
    "topk_results = []\n",
    "\n",
    "for k in topk_values:\n",
    "    hits = 0\n",
    "    total = 0\n",
    "    avg_rank = []\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for act_num in act_nums:\n",
    "        declared_obj = action_declared_obj.get(act_num, \"\")\n",
    "        if not declared_obj or declared_obj not in obj_idx_map:\n",
    "            continue\n",
    "        \n",
    "        total += 1\n",
    "        act_j = act_idx_map[act_num]\n",
    "        \n",
    "        # Get similarity scores for this action across all objectives\n",
    "        scores = [(obj_codes[i], sim_matrix[i, act_j]) for i in range(len(obj_codes))]\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Rank of declared objective\n",
    "        ranked_objs = [s[0] for s in scores]\n",
    "        rank = ranked_objs.index(declared_obj) + 1\n",
    "        avg_rank.append(rank)\n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "        \n",
    "        if rank <= k:\n",
    "            hits += 1\n",
    "    \n",
    "    hit_rate = hits / total if total > 0 else 0\n",
    "    mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0\n",
    "    \n",
    "    topk_results.append({\n",
    "        \"top_k\": k,\n",
    "        \"hit_rate\": round(hit_rate, 4),\n",
    "        \"mrr\": round(mrr, 4),\n",
    "        \"avg_rank\": round(np.mean(avg_rank), 2) if avg_rank else 0,\n",
    "        \"total_actions\": total,\n",
    "        \"hits\": hits,\n",
    "    })\n",
    "\n",
    "topk_df = pd.DataFrame(topk_results)\n",
    "print(topk_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Hit rate\n",
    "axes[0].bar(topk_df[\"top_k\"], topk_df[\"hit_rate\"], color=\"steelblue\", edgecolor=\"navy\")\n",
    "axes[0].axhline(y=1.0, color=\"green\", linestyle=\"--\", alpha=0.5)\n",
    "for i, row in topk_df.iterrows():\n",
    "    axes[0].text(row[\"top_k\"], row[\"hit_rate\"] + 0.02,\n",
    "                 f\"{row['hit_rate']:.1%}\", ha=\"center\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"top_k\")\n",
    "axes[0].set_ylabel(\"Hit Rate\")\n",
    "axes[0].set_title(\"Declared Objective in Top-k Retrieved\")\n",
    "axes[0].set_ylim(0, 1.15)\n",
    "axes[0].set_xticks(topk_values)\n",
    "\n",
    "# MRR\n",
    "axes[1].bar(topk_df[\"top_k\"], topk_df[\"mrr\"], color=\"coral\", edgecolor=\"darkred\")\n",
    "for i, row in topk_df.iterrows():\n",
    "    axes[1].text(row[\"top_k\"], row[\"mrr\"] + 0.02,\n",
    "                 f\"{row['mrr']:.3f}\", ha=\"center\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"top_k\")\n",
    "axes[1].set_ylabel(\"Mean Reciprocal Rank\")\n",
    "axes[1].set_title(\"MRR vs top_k\")\n",
    "axes[1].set_ylim(0, 1.15)\n",
    "axes[1].set_xticks(topk_values)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"exp4_topk_tuning.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Recommendation\n",
    "best_k_idx = topk_df[\"hit_rate\"].idxmax()\n",
    "min_perfect_k = topk_df[topk_df[\"hit_rate\"] >= 0.99][\"top_k\"].min() if (topk_df[\"hit_rate\"] >= 0.99).any() else topk_df.loc[best_k_idx, \"top_k\"]\n",
    "print(f\"\\n★ Recommended top_k: {min_perfect_k} (smallest k with ≥99% hit rate)\")\n",
    "print(f\"  Current top_k: 3\")\n",
    "print(f\"  MRR: {topk_df.loc[0, 'mrr']:.4f} (constant — not affected by k)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary of Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"  ISPS Parameter Tuning — Summary of Results\")\nprint(\"=\" * 70)\n\n# Collect results\nbest_model_name = summary_df[\"AUC\"].idxmax()\nbest_model_auc = summary_df.loc[best_model_name, \"AUC\"]\nbest_model_time = summary_df.loc[best_model_name, \"Total (s)\"]\ncurrent_model_auc = summary_df.loc[\"all-MiniLM-L6-v2\", \"AUC\"] if \"all-MiniLM-L6-v2\" in summary_df.index else \"N/A\"\ncurrent_model_time = summary_df.loc[\"all-MiniLM-L6-v2\", \"Total (s)\"] if \"all-MiniLM-L6-v2\" in summary_df.index else \"N/A\"\nfastest = summary_df[\"Total (s)\"].idxmin()\n\nbest_threshold = sweep_df.loc[sweep_df[\"f1\"].idxmax(), \"threshold\"]\nbest_threshold_f1 = sweep_df[\"f1\"].max()\ncurrent_threshold_f1 = sweep_df[sweep_df[\"threshold\"] == 0.45][\"f1\"].values[0] if 0.45 in sweep_df[\"threshold\"].values else \"N/A\"\n\nprint(f\"\"\"\n┌─────────────────────────────┬───────────────────────────────┬────────────────────┐\n│ Parameter                   │ Current → Optimal             │ Improvement        │\n├─────────────────────────────┼───────────────────────────────┼────────────────────┤\n│ Embedding Model             │ all-MiniLM-L6-v2              │                    │\n│   (best accuracy)           │  → {best_model_name:<26s} │ AUC: {current_model_auc}→{best_model_auc}  │\n│   (performance)             │  time: {current_model_time}s → {best_model_time}s{'':<14s} │ Fastest: {fastest} │\n├─────────────────────────────┼───────────────────────────────┼────────────────────┤\n│ Classification Threshold    │ 0.45 → {best_threshold:<23.2f} │ F1: {current_threshold_f1}→{best_threshold_f1:.4f}│\n├─────────────────────────────┼───────────────────────────────┼────────────────────┤\n│ Ontology Embedding Weight   │ 0.60 → see Exp 3 plot         │ See Exp 3 above    │\n├─────────────────────────────┼───────────────────────────────┼────────────────────┤\n│ RAG top_k                   │ 3 → {min_perfect_k:<25d} │ Hit rate: see Exp4 │\n└─────────────────────────────┴───────────────────────────────┴────────────────────┘\n\"\"\")\n\nprint(\"Performance summary per model:\")\nperf_cols = [\"Dims\", \"Params (M)\", \"Load (s)\", \"Encode (s)\", \"Total (s)\", \"AUC\", \"F1\"]\nprint(summary_df[perf_cols].to_string())\n\nprint(\"\\nTo apply these values, update the constants in:\")\nprint(\"  • src/vector_store.py             → EMBEDDING_MODEL_NAME\")\nprint(\"  • src/synchronization_analyzer.py → THRESHOLD_FAIR, ORPHAN_THRESHOLD\")\nprint(\"  • src/ontology_mapper.py          → EMBEDDING_WEIGHT\")\nprint(\"  • src/rag_engine.py               → top_k in _retrieve_strategic_context()\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}