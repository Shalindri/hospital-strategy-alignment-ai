#!/usr/bin/env python3
"""
Evaluate the quality of improvement suggestions generated by the ISPS system.

Metrics
-------
1. **Relevance** – Embedding cosine-similarity between the suggestion text and the
   target strategic objective text.
2. **Specificity** – 1 − similarity(suggestion, generic_template) so that generic
   boilerplate scores low.
3. **Actionability** – Heuristic score based on presence of concrete indicators:
   KPIs, timelines, budget figures, named owners, measurable targets.
4. **Coherence** – LLM-as-judge score (Ollama llama3.1:8b) rating the suggestion
   on a 1-5 scale for logical consistency and strategic fit.

The script collects suggestions from three ISPS sources:
  • RAG improvements   (data/rag_recommendations.json  → improvements with content)
  • RAG new-action suggestions  (same file → new_action_suggestions)
  • Agent recommendations  (outputs/agent_recommendations.json)

It also constructs *generic baseline templates* for comparison.

Outputs (written to outputs/evaluation/)
  • suggestion_evaluation.json   – per-suggestion scores + aggregates
  • suggestion_comparison.csv    – side-by-side ISPS vs baseline
  • suggestion_scores.png        – grouped bar chart of mean metrics
  • suggestion_radar.png         – radar / spider chart per source
"""

from __future__ import annotations

import json
import math
import os
import re
import sys
import textwrap
from pathlib import Path
from typing import Any

import numpy as np

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
PROJECT = Path(__file__).resolve().parent.parent
DATA    = PROJECT / "data"
OUTPUT  = PROJECT / "outputs"
EVAL_DIR = OUTPUT / "evaluation"

# ---------------------------------------------------------------------------
# Helpers – text cleaning
# ---------------------------------------------------------------------------
_TAG_RE = re.compile(r"\*{0,2}[A-Z_]+:\*{0,2}\s*")

def _clean(text: str) -> str:
    """Strip LLM formatting tags and collapse whitespace."""
    text = _TAG_RE.sub("", text)
    text = re.sub(r"\*{2,}", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


# ---------------------------------------------------------------------------
# Data loaders
# ---------------------------------------------------------------------------
def _load_json(path: Path) -> dict | list:
    with open(path) as f:
        return json.load(f)


def load_strategic_plan() -> dict:
    return _load_json(DATA / "strategic_plan.json")


def load_action_plan() -> dict:
    return _load_json(DATA / "action_plan.json")


def load_rag_recommendations() -> dict:
    return _load_json(DATA / "rag_recommendations.json")


def load_agent_recommendations() -> dict:
    return _load_json(OUTPUT / "agent_recommendations.json")


# ---------------------------------------------------------------------------
# Build objective texts  (objective_code → concatenated description)
# ---------------------------------------------------------------------------
def _build_objective_texts(plan: dict) -> dict[str, str]:
    texts: dict[str, str] = {}
    for obj in plan.get("objectives", []):
        code = obj["code"]
        parts = [obj.get("name", ""), obj.get("goal_statement", "")]
        for sg in obj.get("strategic_goals", []):
            parts.append(sg.get("description", ""))
        for kpi in obj.get("kpis", []):
            parts.append(kpi if isinstance(kpi, str) else kpi.get("description", ""))
        texts[code] = " ".join(p for p in parts if p)
    return texts


# ---------------------------------------------------------------------------
# Collect ISPS suggestions as normalised dicts
# ---------------------------------------------------------------------------

def _action_lookup(action_plan: dict) -> dict[int, dict]:
    return {a["action_number"]: a for a in action_plan["actions"]}


def collect_suggestions(
    rag: dict,
    agent: dict,
    action_plan: dict,
) -> list[dict]:
    """Return list of dicts with keys: source, id, objective, text, kpis."""
    actions = _action_lookup(action_plan)
    suggestions: list[dict] = []

    # 1) RAG improvements that have actual content
    for imp in rag.get("improvements", []):
        desc = _clean(imp.get("modified_description", ""))
        linkage = _clean(imp.get("strategic_linkage", ""))
        if not desc and not linkage:
            continue  # skip empty improvements
        kpis = [k for k in imp.get("additional_kpis", []) if not k.startswith("*")]
        text = f"{desc} {linkage}".strip()
        suggestions.append({
            "source": "rag_improvement",
            "id": f"rag_imp_{imp['action_number']}",
            "action_number": imp["action_number"],
            "action_title": imp.get("action_title", ""),
            "objective": imp.get("declared_objective", ""),
            "text": text,
            "kpis": kpis,
        })

    # 2) RAG new-action suggestions
    for sug in rag.get("new_action_suggestions", []):
        parts = [
            sug.get("title", ""),
            _clean(sug.get("description", "")),
            _clean(sug.get("strategic_linkage", "")),
        ]
        text = " ".join(p for p in parts if p)
        suggestions.append({
            "source": "rag_new_action",
            "id": f"rag_new_{sug['objective_code']}_{sug['suggestion_index']}",
            "action_number": None,
            "action_title": sug.get("title", ""),
            "objective": sug["objective_code"],
            "text": text,
            "kpis": sug.get("kpis", []),
            "timeline": sug.get("timeline", ""),
            "budget": sug.get("budget_estimate", ""),
            "owner": sug.get("owner", ""),
        })

    # 3) Agent recommendations
    for rec in agent.get("recommendations", []):
        parts = [
            _clean(rec.get("what_to_change", "")),
            _clean(rec.get("why", "")),
        ]
        for a in rec.get("actions", []):
            parts.append(_clean(a))
        text = " ".join(p for p in parts if p)
        objs = rec.get("affected_objectives", [])
        suggestions.append({
            "source": "agent_recommendation",
            "id": rec.get("rec_id", ""),
            "action_number": None,
            "action_title": rec.get("issue_id", ""),
            "objective": objs[0] if objs else "",
            "text": text,
            "kpis": [k for k in rec.get("kpis", []) if not k.startswith("*")],
        })

    return suggestions


# ---------------------------------------------------------------------------
# Generic baseline templates
# ---------------------------------------------------------------------------
_BASELINE_TEMPLATES = {
    "A": "Improve patient care by enhancing clinical services and hospital capacity.",
    "B": "Adopt digital health solutions to modernise hospital operations.",
    "C": "Invest in research and innovation to foster academic excellence.",
    "D": "Develop workforce capabilities through recruitment and training.",
    "E": "Expand community health outreach and regional partnerships.",
}


def build_baselines(suggestions: list[dict]) -> list[dict]:
    """Create a generic baseline suggestion for every ISPS suggestion."""
    baselines = []
    for s in suggestions:
        obj = s["objective"]
        baselines.append({
            "source": "baseline_generic",
            "id": f"baseline_{s['id']}",
            "action_number": s.get("action_number"),
            "action_title": s.get("action_title", ""),
            "objective": obj,
            "text": _BASELINE_TEMPLATES.get(obj, "Improve hospital operations."),
            "kpis": [],
        })
    return baselines


# ---------------------------------------------------------------------------
# Metric 1 – Relevance (embedding similarity to strategic objective)
# ---------------------------------------------------------------------------
def compute_relevance(
    suggestions: list[dict],
    objective_texts: dict[str, str],
    model,
) -> list[float]:
    """Cosine similarity between suggestion text and its objective text."""
    from sklearn.metrics.pairwise import cosine_similarity

    scores = []
    for s in suggestions:
        obj_text = objective_texts.get(s["objective"], "")
        if not obj_text or not s["text"]:
            scores.append(0.0)
            continue
        emb = model.encode([s["text"], obj_text])
        sim = float(cosine_similarity([emb[0]], [emb[1]])[0, 0])
        scores.append(max(0.0, sim))
    return scores


# ---------------------------------------------------------------------------
# Metric 2 – Specificity (1 − similarity to generic template)
# ---------------------------------------------------------------------------
def compute_specificity(
    suggestions: list[dict],
    model,
) -> list[float]:
    scores = []
    for s in suggestions:
        generic = _BASELINE_TEMPLATES.get(s["objective"], "Improve hospital operations.")
        if not s["text"]:
            scores.append(0.0)
            continue
        emb = model.encode([s["text"], generic])
        from sklearn.metrics.pairwise import cosine_similarity
        sim = float(cosine_similarity([emb[0]], [emb[1]])[0, 0])
        scores.append(max(0.0, 1.0 - sim))
    return scores


# ---------------------------------------------------------------------------
# Metric 3 – Actionability  (heuristic)
# ---------------------------------------------------------------------------
_ACTIONABILITY_INDICATORS = {
    "has_kpis":     lambda s: len(s.get("kpis", [])) > 0,
    "has_timeline": lambda s: bool(
        s.get("timeline")
        or re.search(r"Q[1-4]\s*20\d{2}", s["text"])
        or re.search(r"\d+\s*(month|week|year)", s["text"], re.I)
    ),
    "has_budget":   lambda s: bool(
        s.get("budget")
        or re.search(r"LKR\s*[\d.]+", s["text"])
        or re.search(r"\d+\s*million", s["text"], re.I)
    ),
    "has_owner":    lambda s: bool(
        s.get("owner")
        or re.search(r"(Director|Manager|Department|CMO|CIO|Head of)", s["text"], re.I)
    ),
    "has_measurable_target": lambda s: bool(
        re.search(r"\d+\s*%", s["text"])
        or re.search(r"target[:\s]+\d", s["text"], re.I)
        or re.search(r"≥\s*\d", s["text"])
    ),
}


def compute_actionability(suggestions: list[dict]) -> list[float]:
    n_indicators = len(_ACTIONABILITY_INDICATORS)
    scores = []
    for s in suggestions:
        hits = sum(1 for fn in _ACTIONABILITY_INDICATORS.values() if fn(s))
        scores.append(hits / n_indicators)
    return scores


# ---------------------------------------------------------------------------
# Metric 4 – Coherence  (LLM-as-judge via Ollama)
# ---------------------------------------------------------------------------
_COHERENCE_PROMPT = textwrap.dedent("""\
    You are evaluating an improvement suggestion for a hospital strategic plan.
    Rate the following suggestion on a scale of 1-5 for COHERENCE, considering:
    - Logical consistency of the recommendation
    - Strategic fit with the stated objective
    - Clarity and completeness of the proposal

    Strategic Objective: {objective}
    Suggestion: {suggestion}

    Respond with ONLY a single integer between 1 and 5.
    1 = Incoherent / irrelevant
    2 = Somewhat relevant but vague
    3 = Moderately coherent
    4 = Coherent and well-structured
    5 = Highly coherent, strategic, and complete
""")


def _query_ollama(prompt: str, model_name: str = "llama3.1:8b") -> str:
    """Call Ollama REST API and return the response text."""
    import urllib.request
    payload = json.dumps({
        "model": model_name,
        "prompt": prompt,
        "stream": False,
        "options": {"temperature": 0.1, "num_predict": 10},
    }).encode()
    req = urllib.request.Request(
        "http://localhost:11434/api/generate",
        data=payload,
        headers={"Content-Type": "application/json"},
    )
    try:
        with urllib.request.urlopen(req, timeout=60) as resp:
            return json.loads(resp.read().decode()).get("response", "")
    except Exception as e:
        print(f"  [warn] Ollama call failed: {e}")
        return ""


def compute_coherence(
    suggestions: list[dict],
    objective_texts: dict[str, str],
    use_llm: bool = True,
) -> list[float]:
    """LLM-as-judge coherence score normalised to 0-1."""
    scores: list[float] = []
    for i, s in enumerate(suggestions):
        if not use_llm:
            scores.append(0.6)  # neutral fallback
            continue
        obj_text = objective_texts.get(s["objective"], s["objective"])
        prompt = _COHERENCE_PROMPT.format(
            objective=obj_text[:500],
            suggestion=s["text"][:800],
        )
        raw = _query_ollama(prompt)
        # Extract first integer 1-5 from response
        m = re.search(r"[1-5]", raw)
        if m:
            score = int(m.group()) / 5.0
        else:
            score = 0.6  # fallback
        scores.append(score)
        print(f"  Coherence [{i+1}/{len(suggestions)}] {s['id']}: {score:.2f}")
    return scores


# ---------------------------------------------------------------------------
# Aggregate and compare
# ---------------------------------------------------------------------------
def evaluate_all(
    suggestions: list[dict],
    objective_texts: dict[str, str],
    model,
    use_llm: bool = True,
) -> list[dict]:
    """Compute all 4 metrics for each suggestion, return enriched dicts."""
    print("Computing relevance scores...")
    relevance    = compute_relevance(suggestions, objective_texts, model)
    print("Computing specificity scores...")
    specificity  = compute_specificity(suggestions, model)
    print("Computing actionability scores...")
    actionability = compute_actionability(suggestions)
    print("Computing coherence scores (LLM-as-judge)...")
    coherence    = compute_coherence(suggestions, objective_texts, use_llm)

    results = []
    for i, s in enumerate(suggestions):
        composite = (
            0.30 * relevance[i]
            + 0.20 * specificity[i]
            + 0.25 * actionability[i]
            + 0.25 * coherence[i]
        )
        results.append({
            **s,
            "relevance": round(relevance[i], 4),
            "specificity": round(specificity[i], 4),
            "actionability": round(actionability[i], 4),
            "coherence": round(coherence[i], 4),
            "composite": round(composite, 4),
        })
    return results


def aggregate_by_source(results: list[dict]) -> dict[str, dict]:
    """Group results by source and compute mean metrics."""
    from collections import defaultdict
    groups: dict[str, list[dict]] = defaultdict(list)
    for r in results:
        groups[r["source"]].append(r)

    agg = {}
    for source, items in groups.items():
        n = len(items)
        agg[source] = {
            "count": n,
            "mean_relevance": round(np.mean([x["relevance"] for x in items]), 4),
            "mean_specificity": round(np.mean([x["specificity"] for x in items]), 4),
            "mean_actionability": round(np.mean([x["actionability"] for x in items]), 4),
            "mean_coherence": round(np.mean([x["coherence"] for x in items]), 4),
            "mean_composite": round(np.mean([x["composite"] for x in items]), 4),
        }
    return agg


# ---------------------------------------------------------------------------
# Visualisations
# ---------------------------------------------------------------------------
def generate_plots(agg: dict[str, dict], eval_dir: Path) -> None:
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt

    metrics = ["mean_relevance", "mean_specificity", "mean_actionability",
               "mean_coherence", "mean_composite"]
    labels = ["Relevance", "Specificity", "Actionability", "Coherence", "Composite"]

    sources = list(agg.keys())
    source_labels = {
        "rag_improvement": "RAG\nImprovements",
        "rag_new_action": "RAG New\nActions",
        "agent_recommendation": "Agent\nRecs",
        "baseline_generic": "Baseline\n(Generic)",
    }

    # --- 1. Grouped bar chart ---
    fig, ax = plt.subplots(figsize=(10, 6))
    x = np.arange(len(sources))
    width = 0.15
    colours = ["#2196F3", "#4CAF50", "#FF9800", "#9C27B0", "#F44336"]

    for j, (metric, label) in enumerate(zip(metrics, labels)):
        vals = [agg[s].get(metric, 0) for s in sources]
        ax.bar(x + j * width, vals, width, label=label, color=colours[j])

    ax.set_xticks(x + width * 2)
    ax.set_xticklabels([source_labels.get(s, s) for s in sources], fontsize=9)
    ax.set_ylabel("Score (0–1)")
    ax.set_title("Suggestion Quality — ISPS vs Baseline")
    ax.set_ylim(0, 1.05)
    ax.legend(loc="upper right", fontsize=8)
    ax.grid(axis="y", alpha=0.3)
    fig.tight_layout()
    fig.savefig(eval_dir / "suggestion_scores.png", dpi=150)
    plt.close(fig)
    print(f"  Saved suggestion_scores.png")

    # --- 2. Radar chart ---
    radar_metrics = metrics[:4]  # exclude composite
    radar_labels = labels[:4]
    N = len(radar_labels)
    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
    angles += angles[:1]

    fig, ax = plt.subplots(figsize=(7, 7), subplot_kw=dict(polar=True))
    radar_colours = ["#2196F3", "#4CAF50", "#FF9800", "#F44336"]

    for i, s in enumerate(sources):
        vals = [agg[s].get(m, 0) for m in radar_metrics]
        vals += vals[:1]
        ax.plot(angles, vals, "o-", label=source_labels.get(s, s),
                color=radar_colours[i % len(radar_colours)], linewidth=2)
        ax.fill(angles, vals, alpha=0.1, color=radar_colours[i % len(radar_colours)])

    ax.set_thetagrids(np.degrees(angles[:-1]), radar_labels)
    ax.set_ylim(0, 1.0)
    ax.set_title("Suggestion Quality Radar", pad=20)
    ax.legend(loc="upper right", bbox_to_anchor=(1.3, 1.1), fontsize=8)
    fig.tight_layout()
    fig.savefig(eval_dir / "suggestion_radar.png", dpi=150)
    plt.close(fig)
    print(f"  Saved suggestion_radar.png")


# ---------------------------------------------------------------------------
# CSV export
# ---------------------------------------------------------------------------
def export_csv(results: list[dict], path: Path) -> None:
    import csv
    fieldnames = [
        "source", "id", "objective", "action_title",
        "relevance", "specificity", "actionability", "coherence", "composite",
    ]
    with open(path, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
        writer.writeheader()
        writer.writerows(results)
    print(f"  Saved {path.name}")


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------
def run_evaluation(use_llm: bool = True) -> dict:
    """Full evaluation pipeline. Returns the results dict."""
    EVAL_DIR.mkdir(parents=True, exist_ok=True)

    # Load data
    print("Loading data...")
    strategic_plan = load_strategic_plan()
    action_plan    = load_action_plan()
    rag            = load_rag_recommendations()
    agent          = load_agent_recommendations()

    objective_texts = _build_objective_texts(strategic_plan)
    print(f"  Objectives loaded: {list(objective_texts.keys())}")

    # Load embedding model
    print("Loading sentence-transformers model...")
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer("all-MiniLM-L6-v2")

    # Collect ISPS suggestions
    isps_suggestions = collect_suggestions(rag, agent, action_plan)
    print(f"  ISPS suggestions collected: {len(isps_suggestions)}")
    for src in set(s["source"] for s in isps_suggestions):
        n = sum(1 for s in isps_suggestions if s["source"] == src)
        print(f"    {src}: {n}")

    # Build baseline
    baselines = build_baselines(isps_suggestions)
    print(f"  Baseline suggestions: {len(baselines)}")

    # Evaluate ISPS
    print("\n=== Evaluating ISPS suggestions ===")
    isps_results = evaluate_all(isps_suggestions, objective_texts, model, use_llm)

    # Evaluate baselines
    print("\n=== Evaluating Baseline suggestions ===")
    baseline_results = evaluate_all(baselines, objective_texts, model, use_llm=False)

    # Aggregate
    all_results = isps_results + baseline_results
    agg = aggregate_by_source(all_results)

    # Summary
    print("\n=== Aggregated Results ===")
    for source, metrics in agg.items():
        print(f"  {source}:")
        for k, v in metrics.items():
            print(f"    {k}: {v}")

    # Save JSON
    output = {
        "per_suggestion": [
            {k: v for k, v in r.items() if k != "text"}
            for r in all_results
        ],
        "aggregated": agg,
        "isps_vs_baseline": {},
    }

    # Compute ISPS aggregate vs baseline
    isps_agg_keys = [k for k in agg if k != "baseline_generic"]
    if isps_agg_keys and "baseline_generic" in agg:
        isps_combined = {
            "count": sum(agg[k]["count"] for k in isps_agg_keys),
        }
        for metric in ["mean_relevance", "mean_specificity",
                        "mean_actionability", "mean_coherence", "mean_composite"]:
            vals = [agg[k][metric] for k in isps_agg_keys]
            weights = [agg[k]["count"] for k in isps_agg_keys]
            isps_combined[metric] = round(
                sum(v * w for v, w in zip(vals, weights)) / sum(weights), 4
            )
        baseline_metrics = agg["baseline_generic"]
        improvement = {}
        for metric in ["mean_relevance", "mean_specificity",
                        "mean_actionability", "mean_coherence", "mean_composite"]:
            diff = isps_combined[metric] - baseline_metrics[metric]
            improvement[metric] = round(diff, 4)

        output["isps_vs_baseline"] = {
            "isps_weighted_avg": isps_combined,
            "baseline": baseline_metrics,
            "improvement": improvement,
        }

    json_path = EVAL_DIR / "suggestion_evaluation.json"
    with open(json_path, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\n  Saved {json_path.name}")

    # Save CSV
    export_csv(all_results, EVAL_DIR / "suggestion_comparison.csv")

    # Generate plots
    print("\nGenerating plots...")
    generate_plots(agg, EVAL_DIR)

    # Print headline comparison
    if output["isps_vs_baseline"]:
        print("\n" + "=" * 60)
        print("ISPS vs Baseline (Generic Templates)")
        print("=" * 60)
        imp = output["isps_vs_baseline"]["improvement"]
        for k, v in imp.items():
            label = k.replace("mean_", "").capitalize()
            sign = "+" if v > 0 else ""
            print(f"  {label:15s}: {sign}{v:.4f}")

    return output


if __name__ == "__main__":
    use_llm = "--no-llm" not in sys.argv
    if not use_llm:
        print("Running without LLM coherence scoring (--no-llm flag)")
    run_evaluation(use_llm=use_llm)
